import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter  # â† ì—¬ê¸° ìˆ˜ì •!
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import os

# =====================
# í˜ì´ì§€ ì„¤ì •
# =====================
st.set_page_config(
    page_title="PDF ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="centered"
)

# =====================
# ìŠ¤íƒ€ì¼ ì ìš©
# =====================
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 0.5rem;
    }
    .main-header {
        text-align: center;
        padding: 1rem 0;
        border-bottom: 2px solid #f0f2f6;
        margin-bottom: 2rem;
    }
    .status-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f0f2f6;
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# =====================
# í—¤ë”
# =====================
st.markdown('<div class="main-header">', unsafe_allow_html=True)
st.title("ğŸ“š PDF ë¬¸ì„œ ì±—ë´‡")
st.caption("test.pdf ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤")
st.markdown('</div>', unsafe_allow_html=True)

# =====================
# API í‚¤ ì„¤ì •
# =====================
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# =====================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =====================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chain" not in st.session_state:
    st.session_state.chain = None

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

# =====================
# RAG ì²´ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
# =====================
@st.cache_resource
def initialize_rag_chain():
    """PDF ë¡œë“œ ë° RAG ì²´ì¸ êµ¬ì„±"""
    
    # PDF íŒŒì¼ ê²½ë¡œ
    pdf_path = "test.pdf"
    
    if not os.path.exists(pdf_path):
        st.error(f"âš ï¸ {pdf_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        # 1. PDF ë¡œë“œ
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        
        # 2. í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        splits = text_splitter.split_documents(documents)
        
        # 3. ì„ë² ë”© ìƒì„±
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=GEMINI_API_KEY
        )
        
        # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(splits, embeddings)
        
        # 5. LLM ì„¤ì •
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=GEMINI_API_KEY,
            temperature=0.3,
            convert_system_message_to_human=True
        )
        
        # 6. ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            memory=st.session_state.memory,
            return_source_documents=True,
            output_key="answer"
        )
        
    return chain

# =====================
# RAG ì²´ì¸ ë¡œë“œ
# =====================
if st.session_state.chain is None:
    st.session_state.chain = initialize_rag_chain()

# ì²´ì¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
if st.session_state.chain is None:
    st.stop()

# ë¡œë“œ ì„±ê³µ í‘œì‹œ
st.success("âœ… PDF ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

# =====================
# ì‚¬ì´ë“œë°”
# =====================
with st.sidebar:
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    1. ì•„ë˜ ì±„íŒ…ì°½ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
    2. PDF ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
    3. ëŒ€í™” ê¸°ë¡ì´ ìœ ì§€ë©ë‹ˆë‹¤
    """)
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.messages = []
        st.session_state.memory.clear()
        st.rerun()
    
    st.divider()
    st.caption("Powered by Gemini 2.5 Flash")

# =====================
# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
# =====================
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# =====================
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# =====================
if prompt := st.chat_input("PDF ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            try:
                response = st.session_state.chain.invoke({"question": prompt})
                answer = response["answer"]
                
                # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ê°€ (ì„ íƒì )
                if response.get("source_documents"):
                    sources = response["source_documents"]
                    unique_pages = set([doc.metadata.get("page", 0) + 1 for doc in sources])
                    source_info = f"\n\n---\nğŸ“– *ì°¸ì¡° í˜ì´ì§€: {', '.join(map(str, sorted(unique_pages)))}*"
                    answer += source_info
                
                st.markdown(answer)
                
                # ì‘ë‹µ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                error_msg = f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
